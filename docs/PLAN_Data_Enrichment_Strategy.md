# خطة إثراء بيانات المعجم بدقة عالية (High-Precision Lexicon Enrichment Plan)

## 1. الملخص التنفيذي (Executive Summary)

الهدف هو تحويل ملف `translations-data.json` الذي يحتوي على ما يقرب من 14,000 مدخل من مجرد "ترجمات بسيطة" إلى **قاعدة بيانات لغوية غنية (Rich Linguistic Database)**. نظراً لضخامة البيانات ومتطلبات الدقة الشديدة، لا يمكن تنفيذ ذلك يدوياً أو في جلسة واحدة. تتضمن هذه الخطة استراتيجية **"الطيار ثم الأتمتة" (Pilot-then-Automate)**.

---

## 2. التحديات (Challenges)

1. **حجم البيانات:** 14,000 كلمة × (تعريفات + أمثلة + تصريفات + نطق) = حجم نصي ضخم جداً.
2. **تعدد المعاني (Polysemy):** الكلمة الواحدة قد تحتاج للانقسام إلى عدة "مفاهيم" (Concepts).
3. **الدقة (Accuracy):** الحاجة إلى بيانات موثوقة لغوياً (IPA صحيح، تصريفات دقيقة).
4. **التكلفة والوقت:** المعالجة اليدوية مستحيلة؛ المعالجة الآلية تتطلب إدارة ذكية للأخطاء (Error Handling).

---

## 3. استراتيجية التنفيذ (Execution Strategy)

### المرحلة الأولى: تحديد "المعيار الذهبي" (The Gold Standard Pilot)

**الهدف:** الخروج بـ 50 كلمة نموذجية تمثل الجودة المطلوبة تماماً.

* **الإجراء:** سنقوم باختيار أول 50 كلمة من الملف ومعالجتها يدوياً/تفاعلياً للتأكد من صحة:
  * بنية JSON.
  * دقة وجودة التعريفات (Definitions).
  * صحة النطق (IPA) لكل اللغات.
  * جودة الأمثلة والسياق.
* **المخرج:** ملف `gold_standard_sample.json` نستخدمه كمرجع (Few-Shot Examples) للذكاء الاصطناعي في المرحلة التالية.

### المرحلة الثانية: بناء خط الأنابيب الآلي (Automated Pipeline Development)

**الهدف:** كتابة سكريبت (Python Script) يقوم بمحاكاة "الخبير اللغوي".

* **الأدوات:** Python + LLM API (OpenAI GPT-4o or Gemini 1.5 Pro).
* **تصميم السكريبت (`enrich_lexicon.py`):**
  1. **القراءة:** قراءة الملف الأصلي سطرًا تلو الآخر.
  2. **المعالجة (Processing):** إرسال الكلمة + بياناتها الحالية إلى النموذج الذكي مع `System Prompt` صارم جداً يحتوي على "المعيار الذهبي" كأمثلة.
  3. **التحقق (Validation):** كود يتحقق فوراً من أن الرد بصيغة JSON صحيحة ويحتوي على الحقول المطلوبة (concept_id, pronunciation, inflections).
  4. **الحفظ (Saving):** الحفظ في ملف `enriched_data.jsonl` (سطر بسطر) لضمان عدم ضياع البيانات في حال انقطاع الاتصال.

### المرحلة الثالثة: الهندسة العكسية للموجهات (Prompt Engineering)

لضمان "الدقة الشديدة"، سنصمم الموجه (Prompt) ليشمل قواعد صارمة:

* "لا تخمن النطق، استخدم IPA الموحد".
* "إذا كانت الكلمة تحتمل معنيين مختلفين جداً (مثل Bank: ضفة/بنك)، قم بفصلهم إلى Concept Objects منفصلة".
* "الأمثلة يجب أن تكون جمل طبيعية ومستخدمة حديثاً".

### المرحلة الرابعة: التشغيل والمراقبة (Execution & Monitoring)

* تقسيم الـ 14,000 كلمة إلى دفعات (Batches)، كل دفعة 500 كلمة.
* تشغيل السكريبت ومراجعة عينة عشوائية (5%) من كل دفعة يدوياً.
* دمج النتائج النهائية في ملف `translations-data-final.json`.

---

## 4. نموذج البيانات المستهدف (Target Schema)

هذا هو الشكل النهائي الذي سيتم تحويل كل سطر بسيط إليه:

```json
{
  "concept_id": "CNT-10001-V1",
  "word_family_root": "book",
  "definition": "A written or printed work consisting of pages...",
  "part_of_speech": "noun",
  "lexical_entries": {
    "en": {
      "lemma": "book",
      "pronunciations": [{ "ipa": "/bʊk/", "audio": "..." }],
      "inflections": [
        { "form": "books", "tags": ["plural"] }
      ],
      "examples": ["I read a book."]
    },
    "ar": {
      "lemma": "كتاب",
      "pronunciations": [{ "ipa": "/kiˈtaːb/" }],
      "inflections": [
        { "form": "كتب", "tags": ["plural", "broken_plural"] }
      ],
      "examples": ["قرأت كتاباً ممتعاً."]
    },
    "fr": { ... },
    "de": { ... }
  },
  "relations": {
    "synonyms": ["volume", "publication"],
    "domain": "General"
  }
}
```

## 5. الخطوة التالية (Next Actions)

1. **المصادقة:** هل توافق على هذا الهيكل وهذه الخطة؟
2. **البدء:** سأقوم بإنشاء `data_enrichment_pipeline.py` والبدء في "المرحلة الأولى" (عينة الـ 200 كلمة) فوراً.
